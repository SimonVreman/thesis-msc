import {
  PrismaClient,
  type Trace,
  type VirtualMachine,
} from "../generated/prisma";
import { read, readChunked } from "./csvgz";

const which = "cpu-sql" as "vm" | "cpu" | "cpu-sql";

const paths = {
  cpu: (n: number) => `./traces/cpu/vm_cpu_readings-file-${n}-of-195.csv.gz`,
  vm: "./traces/vmtable.csv.gz",
} as const;

const idx = {
  cpu: { t: 0, vm: 1, min: 2, max: 3, avg: 4 },
  vm: {
    id: 0,
    subId: 1,
    depId: 2,
    tCreate: 3,
    tDelete: 4,
    max: 5,
    avg: 6,
    p95Max: 7,
    cat: 8,
    core: 9,
    mem: 10,
  },
};

const vmLine = (line: string) => {
  const [id, subId, depId, tCreate, tDelete, max, avg, p95Max, cat, core, mem] =
    line.split(",");
  return {
    id,
    subId,
    depId,
    tCreate,
    tDelete,
    max,
    avg,
    p95Max,
    cat,
    core,
    mem,
  };
};

const cpuLine = (line: string) => {
  const [t, vm, min, max, avg] = line.split(",");
  return { t, vm, min, max, avg };
};

const timeOffset = new Date(2019, 9, 1).getTime() / 1000;
const prisma = new PrismaClient();

let chunk = 0;
let time = performance.now();

if (which === "vm")
  await readChunked({
    path: paths.vm,
    chunkSize: 1e5,
    callback: async (lines) => {
      const currentChunk = ++chunk;
      console.log(
        "processing chunk",
        currentChunk,
        "of",
        Math.ceil(2695548 / 1e5)
      );
      await prisma.virtualMachine.createMany({
        skipDuplicates: true,
        data: lines.map((line) => {
          const vm = vmLine(line);
          return {
            id: vm.id,
            subscription: vm.subId,
            deployment: vm.depId,
            created: new Date((+vm.tCreate + timeOffset) * 1000),
            deleted: new Date((+vm.tDelete + timeOffset) * 1000),
            maxCpu: +vm.max,
            avgCpu: +vm.avg,
            p95MaxCpu: +vm.p95Max,
            category: vm.cat,
            cores: vm.core === ">24" ? 30 : +vm.mem,
            memory: vm.mem === ">64" ? 70 : +vm.mem,
          } satisfies VirtualMachine;
        }),
      });
      console.log("chunk", currentChunk, "done");
    },
  });

// File three in chunks of 1e4 took 459 seconds
// File four in chunks of 1e5 took 470 seconds
// Did upto and including: file 4
if (which === "cpu") {
  const file = paths.cpu(4);
  const chunkSize = 1e5;
  const chunkCount = Math.ceil(10_000_000 / chunkSize);

  await readChunked({
    path: file,
    chunkSize,
    callback: async (lines) => {
      const currentChunk = ++chunk;

      console.log("processing chunk", currentChunk, "of", chunkCount);

      await prisma.trace.createMany({
        skipDuplicates: true,
        data: lines
          .map((line) => {
            const trace = cpuLine(line);

            if (!trace.vm) return null; // skip traces without VM

            return {
              time: new Date((+trace.t + timeOffset) * 1000),
              virtualMachine: trace.vm,
              min: +trace.min,
              max: +trace.max,
              avg: +trace.avg,
            } satisfies Trace;
          })
          .filter((t) => t !== null),
      });
      console.log("chunk", currentChunk, "done");
    },
  });

  console.log("finished reading all chunks of", file);
}

if (which === "cpu-sql") {
  const fileId = 5;
  const file = paths.cpu(fileId);
  const chunkSize = 1e5;
  const chunkCount = Math.ceil(10_000_000 / chunkSize);

  const output = Bun.file(`./sql/cpu.sql`);
  output.write(
    `-- File id ${fileId}. This file was generated by mcp/lib/import.ts\n`
  );
  const writer = output.writer();

  await readChunked({
    path: file,
    chunkSize,
    callback: async (lines) => {
      const currentChunk = ++chunk;

      writer.write(
        'INSERT INTO "Trace" (time,"virtualMachine",min,max,avg) VALUES\n'
      );

      console.log("processing chunk", currentChunk, "of", chunkCount);

      for (let i = 0; i < lines.length; i++) {
        const line = lines[i];
        const trace = cpuLine(line);

        if (!trace.vm) continue; // skip traces without VM

        writer.write(
          `('${new Date((+trace.t + timeOffset) * 1000).toISOString()}','${
            trace.vm
          }',${+trace.min},${+trace.max},${+trace.avg})${
            i < lines.length - 1 ? "," : ";"
          }\n`
        );
      }

      console.log("chunk", currentChunk, "done");
    },
  });

  await writer.end();

  console.log("finished reading all chunks of", file);
}

console.log(
  `completed ${which} data in ${Math.round(
    (performance.now() - time) / 1000
  )} seconds`
);

// DIRECT IMPORT
// su postgres
// psql
// \c thesis
// \copy "traces-import" from program 'zcat /tmp/vm_cpu_readings-file-9-of-195.csv.gz' with (format csv, delimiter ',', header FALSE)
